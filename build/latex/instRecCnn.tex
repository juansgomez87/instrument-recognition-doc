%% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax

\usepackage[utf8]{inputenc}
\ifdefined\DeclareUnicodeCharacter
 \ifdefined\DeclareUnicodeCharacterAsOptional
  \DeclareUnicodeCharacter{"00A0}{\nobreakspace}
  \DeclareUnicodeCharacter{"2500}{\sphinxunichar{2500}}
  \DeclareUnicodeCharacter{"2502}{\sphinxunichar{2502}}
  \DeclareUnicodeCharacter{"2514}{\sphinxunichar{2514}}
  \DeclareUnicodeCharacter{"251C}{\sphinxunichar{251C}}
  \DeclareUnicodeCharacter{"2572}{\textbackslash}
 \else
  \DeclareUnicodeCharacter{00A0}{\nobreakspace}
  \DeclareUnicodeCharacter{2500}{\sphinxunichar{2500}}
  \DeclareUnicodeCharacter{2502}{\sphinxunichar{2502}}
  \DeclareUnicodeCharacter{2514}{\sphinxunichar{2514}}
  \DeclareUnicodeCharacter{251C}{\sphinxunichar{251C}}
  \DeclareUnicodeCharacter{2572}{\textbackslash}
 \fi
\fi
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amstext}
\usepackage{babel}
\usepackage{times}
\usepackage[Bjarne]{fncychap}
\usepackage[dontkeepoldnames]{sphinx}

\usepackage{geometry}

% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}

\addto\captionsenglish{\renewcommand{\figurename}{Fig.}}
\addto\captionsenglish{\renewcommand{\tablename}{Table}}
\addto\captionsenglish{\renewcommand{\literalblockname}{Listing}}

\addto\captionsenglish{\renewcommand{\literalblockcontinuedname}{continued from previous page}}
\addto\captionsenglish{\renewcommand{\literalblockcontinuesname}{continues on next page}}

\addto\extrasenglish{\def\pageautorefname{page}}

\setcounter{tocdepth}{1}



\title{instRecCnn Documentation}
\date{May 05, 2018}
\release{1.0}
\author{Juan Sebastián Gómez Canón}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{Release}
\makeindex

\begin{document}

\maketitle
\sphinxtableofcontents
\phantomsection\label{\detokenize{index::doc}}


Juan Sebastián Gómez Cañón (Ilmenau, Germany - 2018)


\chapter{Introduction}
\label{\detokenize{index:introduction}}\label{\detokenize{index:automatic-instrument-recognition-with-deep-convolutional-neural-networks}}
In the context of growing digital media and new classification/indexing demands, the
task of Automatic Instrument Recognition in the field of Music Information Retrieval
(MIR) has increasing importance. Through the use of deep learning techniques, namely
convolutional neural networks, and different automatic source separation algorithms,
developed at the Fraunhofer Institut für Digitale Medientechnologie (IDMT) %
\begin{footnote}[1]\sphinxAtStartFootnote
Estefanía Cano, Mark D. Plumbley, and Christian Dittmar, \sphinxstyleemphasis{“Phase-based harmonic/percussive separation,”} in Proceedings of the Annual Conference of the International Speech Communication Association (INTERSPEECH), Singapore, 2014, pp. 1628-1632.
%
\end{footnote} %
\begin{footnote}[2]\sphinxAtStartFootnote
Estefanía Cano, Gerald Schuller, and Christian Dittmar, \sphinxstyleemphasis{“Pitch-informed solo and accompaniment separation towards its use in music education applications,”} EURASIP Journal on Advances in Signal Processing, vol. 2014, pp. 23, 2014.
%
\end{footnote}, this
Master thesis investigates this recognition task and how different pre-processing stages
can improve its classification performance. Several experiments have been conducted
in order to reproduce and improve upon the results of the reference system reported
by Han et al. %
\begin{footnote}[3]\sphinxAtStartFootnote
Yoonchang Han, Jaehun Kim, and Kyogu Lee, \sphinxstyleemphasis{“Deep convolutional neural networks for predominant instrument recognition in polyphonic music,”} IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 25, no. 1, pp. 208-221, Jan 2017.
%
\end{footnote} . Two systems are proposed in this research: an improved system using
harmonic/percussive separation and post-processing using class-wise thresholding,
and a combined system using solo/accompaniment separation and transfer learning
methods for the special use case of jazz solo recognition. To validate the obtained
results, diverse tests have been performed with multiple music data sets, with different
complexities and instrument selections.


\chapter{Getting started}
\label{\detokenize{index:getting-started}}

\section{Installation}
\label{\detokenize{installing:installation}}\label{\detokenize{installing::doc}}
This implementation uses the \sphinxstyleemphasis{librosa} python package in order to extract the melspectrograms
that are used to train the neural network. \sphinxstyleemphasis{Pandas} is used to organize the results in
dataframes and evaluate them. \sphinxstyleemphasis{Keras} is the neural networks API used for the implementation
of all the models. Finally, \sphinxstyleemphasis{matplotlib} is used to plot the final results of the performance
metrics obtained. Additionally, the implementation was created in Python 3.


\subsection{pypi}
\label{\detokenize{installing:pypi}}
To install the necessary packages, you can run the following command:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sudo} \PYG{n}{pip} \PYG{n}{install} \PYG{n}{librosa}\PYG{p}{,} \PYG{n}{numpy}\PYG{p}{,} \PYG{n}{pandas}\PYG{p}{,} \PYG{n}{keras}\PYG{p}{,} \PYG{n}{matplotlib}
\end{sphinxVerbatim}


\subsection{TensorFlow}
\label{\detokenize{installing:tensorflow}}
Please refer to this \sphinxhref{https://www.tensorflow.org/install/}{link} to install
TensorFlow depending on your operating system.


\section{Settings}
\label{\detokenize{settings:settings}}\label{\detokenize{settings::doc}}
The file \sphinxstyleemphasis{settings.py} contains all the parameters that have been used based on the paper
by Han et al. You can edit this file to the desired parameters, data sets, and paths.


\subsection{Parameters}
\label{\detokenize{settings:parameters}}\begin{itemize}
\item {} 
\sphinxstylestrong{time\_duration:} duration of spectrogram patches in seconds (default: 1).

\item {} 
\sphinxstylestrong{test\_overlap:} overlap used when extracting the melspectrograms of the testing data sets (default: 0.5).

\item {} 
\sphinxstylestrong{num\_mel\_bands:} number of mel bands to use in the linear to mel frequency transformation (default: 128).

\item {} 
\sphinxstylestrong{nb\_epochs:} maximum number of epochs to perform during training (default: 400).

\item {} 
\sphinxstylestrong{batch\_size:} mini-batch size (default: 128).

\item {} 
\sphinxstylestrong{learning\_rate:} learning rate used for optimization during backpropagation (default: 0.001).

\item {} 
\sphinxstylestrong{alpha:} alpha parameter used for leaky rectified linear units (LReLU) (default: 0.33).

\item {} 
\sphinxstylestrong{iter\_num:} number of training experiments to perform and evaluate (default: 3).

\item {} 
\sphinxstylestrong{nb\_classes:} number of classes for each data set.

\item {} 
\sphinxstylestrong{val\_split:} train-validation split (default: 0.85).

\end{itemize}


\subsection{Platform}
\label{\detokenize{settings:platform}}
By using the \sphinxstyleemphasis{platform} python package, this script automatically selects the corresponding
path for the data sets and the extracted features.


\subsection{Create Metadata}
\label{\detokenize{settings:create-metadata}}
To extract the metadata of the data set and create CSV files, run the command:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{python3} \PYG{n}{create\PYGZus{}encoding}\PYG{o}{.}\PYG{n}{py}
\end{sphinxVerbatim}

At this moment, it is necessary to select the data set to be used during training and testing
of the convolutional neural network. Since several experiments have been conducted, please refer
to the thesis in sections \sphinxstylestrong{5.4 - 5.7} (Proposed Experiments). The currently available data sets
are the following:
\begin{itemize}
\item {} 
{[}-2{]} \sphinxstylestrong{Jazz-Solo} - Jazz data set pre-processed with the solo-accompaniment source separation algorithm \sphinxstyleemphasis{(solo component)}.

\item {} 
{[}-1{]} \sphinxstylestrong{JazzDB} - Jazz data set with no pre-processing.

\item {} 
{[}0{]} \sphinxstylestrong{IRMAS Multires} - IRMAS data set with three dimensional melspectrograms as input (note: this data set must be used with the \sphinxstyleemphasis{model\_multi\_res} network).

\item {} 
{[}1{]} \sphinxstylestrong{IRMAS} - IRMAS data set with no pre-processing.

\item {} 
{[}2{]} \sphinxstylestrong{Monotimbral} - Monotimbral data set with no pre-processing.

\item {} 
{[}3{]} \sphinxstylestrong{IRMAS Harm} - IRMAS data set pre-processed with the harmonic-percussive source separation algorithm \sphinxstyleemphasis{(harmonic component)}.

\item {} 
{[}4{]} \sphinxstylestrong{Monotimbral Harm} - Monotimbral data set pre-processed with the harmonic-percussive source separation algorithm \sphinxstyleemphasis{(harmonic component)}.

\item {} 
{[}5{]} \sphinxstylestrong{Monotimbral Perc} - Monotimbral data set pre-processed with the harmonic-percussive source separation algorithm \sphinxstyleemphasis{(percussive component)}.

\item {} 
{[}6{]} \sphinxstylestrong{IRMAS Wind Solo} - IRMAS Wind data set pre-processed with the solo-accompaniment source separation algorithm \sphinxstyleemphasis{(solo component)}.

\item {} 
{[}7{]} \sphinxstylestrong{IRMAS Wind} - IRMAS Wind data set with no pre-processing.

\item {} 
{[}8{]} \sphinxstylestrong{IRMAS Harm-Perc} - IRMAS data set pre-processed with the harmonic-percussive source separation algorithm \sphinxstyleemphasis{(harmonic and percussive components)} (note: this data set must be used with the \sphinxstyleemphasis{model\_two\_branch} network).

\item {} 
{[}9{]} \sphinxstylestrong{Monotimbral Harm-Perc} - Monotimbral data set pre-processed with the harmonic-percussive source separation algorithm \sphinxstyleemphasis{(harmonic and percussive components)} (note: this data set must be used with the \sphinxstyleemphasis{model\_two\_branch} network).

\end{itemize}

Please ignore the second selector at this stage.


\chapter{Data sets and Training}
\label{\detokenize{index:data-sets-and-training}}

\section{Generate data set}
\label{\detokenize{generate_dataset:generate-data-set}}\label{\detokenize{generate_dataset::doc}}

\subsection{Usage}
\label{\detokenize{generate_dataset:usage}}
To extract the features for a data set and save the resulting tensors, make sure that the
ground truth metadata has been previously created by using the \sphinxstyleemphasis{create\_encoding.py} script.

Given the amount of processing, this class uses the \sphinxstyleemphasis{multiprocessing} package and it is
recommended to run on a GPU, using the following command:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{python3} \PYG{n}{generate\PYGZus{}dataset}\PYG{o}{.}\PYG{n}{py}
\end{sphinxVerbatim}


\subsection{Documentation}
\label{\detokenize{generate_dataset:documentation}}\phantomsection\label{\detokenize{generate_dataset:module-generate_dataset}}\index{generate\_dataset (module)}\index{GenerateDataset (class in generate\_dataset)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{generate_dataset:generate_dataset.GenerateDataset}}\pysiglinewithargsret{\sphinxbfcode{class }\sphinxcode{generate\_dataset.}\sphinxbfcode{GenerateDataset}}{\emph{duration}, \emph{num\_classes}, \emph{val\_split}}{}
GenerateDataset generates training and testing tensors for a given dataset.
The training inputs to the neural network are melspectrograms with 128 
mel bands and variable segment duration length. This class also performs a 
balanced train-validation split depending on the amount of samples in each
class. All of the parameters are inherited from \sphinxstyleemphasis{settings.py}.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{duration} (\sphinxstyleliteralemphasis{int}) \textendash{} number of time bins to create melspectrogram.

\item {} 
\sphinxstyleliteralstrong{num\_classes} (\sphinxstyleliteralemphasis{int}) \textendash{} number of classes to be evaluated.

\item {} 
\sphinxstyleliteralstrong{val\_split} (\sphinxstyleliteralemphasis{float}) \textendash{} training-validation split from 0 to 1.

\end{itemize}

\end{description}\end{quote}
\index{create\_multi\_spectrogram() (generate\_dataset.GenerateDataset method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{generate_dataset:generate_dataset.GenerateDataset.create_multi_spectrogram}}\pysiglinewithargsret{\sphinxbfcode{create\_multi\_spectrogram}}{\emph{filename}, \emph{sr=22050}, \emph{win\_length=1024}, \emph{hop\_length=512}, \emph{num\_mel=128}}{}
This method creates a melspectrogram from an audio file using librosa
audio processing library. Parameters are default from Han et al. It also 
extracts three spectrograms with different window sizes (multiples of the 
original window size) and stacks them into a three-dimensional representation 
of the audio.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{filename} (\sphinxstyleliteralemphasis{str}) \textendash{} wav filename to process.

\item {} 
\sphinxstyleliteralstrong{sr} (\sphinxstyleliteralemphasis{int}) \textendash{} sampling rate in Hz (default: 22050).

\item {} 
\sphinxstyleliteralstrong{win\_length} (\sphinxstyleliteralemphasis{int}) \textendash{} window length for STFT (default: 1024).

\item {} 
\sphinxstyleliteralstrong{hop\_length} (\sphinxstyleliteralemphasis{int}) \textendash{} hop length for STFT (default: 512).

\item {} 
\sphinxstyleliteralstrong{num\_mel} (\sphinxstyleliteralemphasis{int}) \textendash{} number of mel bands (default:128).

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxstylestrong{ln\_S} \sphinxstyleemphasis{(np.array)} - melspectrogram of the complete audio file with logarithmic compression with dimensionality {[}mel bands x time frames x 3{]}.

\end{description}\end{quote}

\end{fulllineitems}

\index{create\_spectrogram() (generate\_dataset.GenerateDataset method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{generate_dataset:generate_dataset.GenerateDataset.create_spectrogram}}\pysiglinewithargsret{\sphinxbfcode{create\_spectrogram}}{\emph{filename}, \emph{sr=22050}, \emph{win\_length=1024}, \emph{hop\_length=512}, \emph{num\_mel=128}}{}
This method creates a melspectrogram from an audio file using librosa
audio processing library. Parameters are default from Han et al.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{filename} (\sphinxstyleliteralemphasis{str}) \textendash{} wav filename to process.

\item {} 
\sphinxstyleliteralstrong{sr} (\sphinxstyleliteralemphasis{int}) \textendash{} sampling rate in Hz (default: 22050).

\item {} 
\sphinxstyleliteralstrong{win\_length} (\sphinxstyleliteralemphasis{int}) \textendash{} window length for STFT (default: 1024).

\item {} 
\sphinxstyleliteralstrong{hop\_length} (\sphinxstyleliteralemphasis{int}) \textendash{} hop length for STFT (default: 512).

\item {} 
\sphinxstyleliteralstrong{num\_mel} (\sphinxstyleliteralemphasis{int}) \textendash{} number of mel bands (default:128).

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxstylestrong{ln\_S} \sphinxstyleemphasis{(np.array)} - melspectrogram of the complete audio file with logarithmic compression with dimensionality {[}mel bands x time frames{]}.

\end{description}\end{quote}

\end{fulllineitems}

\index{load\_metadata() (generate\_dataset.GenerateDataset method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{generate_dataset:generate_dataset.GenerateDataset.load_metadata}}\pysiglinewithargsret{\sphinxbfcode{load\_metadata}}{\emph{path\_metadata}}{}
This method loads the metadata from the dataset previously generated 
by \sphinxstyleemphasis{create\_encoding.py}.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{path\_metadata} (\sphinxstyleliteralemphasis{str}) \textendash{} path to csv with filenames and labels.

\item[{Returns}] \leavevmode
\begin{itemize}
\item {} 
\sphinxstylestrong{filenames} \sphinxstyleemphasis{(list)} - list of filenames that exist in the metadata.

\item {} 
\sphinxstylestrong{labels} \sphinxstyleemphasis{(list)} - list of labels per filename that exist in the metadata.

\end{itemize}


\end{description}\end{quote}

\end{fulllineitems}

\index{pure\_development\_split() (generate\_dataset.GenerateDataset method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{generate_dataset:generate_dataset.GenerateDataset.pure_development_split}}\pysiglinewithargsret{\sphinxbfcode{pure\_development\_split}}{\emph{dataset}, \emph{file\_id}}{}
This method creates balanced 50-50 split between pure testing data and
development testing data from the test data set, depending on 
the total number of examples from each class. This method is only available
for testing data sets and can manage to output a single tensor (for the normal model - multi\_input = \sphinxstylestrong{False}) and double tensors
(for the model with two branches - multi\_input = \sphinxstylestrong{True}).
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{dataset} (\sphinxstyleliteralemphasis{list}) \textendash{} list of extracted spectrograms for test data set.

\item {} 
\sphinxstyleliteralstrong{file\_id} (\sphinxstyleliteralemphasis{list}) \textendash{} file identifier from which the spectrogram was extracted.

\end{itemize}

\item[{Returns}] \leavevmode
\begin{itemize}
\item {} 
\sphinxstylestrong{pure\_test\_set} \sphinxstyleemphasis{(np.array)} - array(s) with spectrograms of pure test data set.

\item {} 
\sphinxstylestrong{fid\_pure\_set} \sphinxstyleemphasis{(list)} - list(s) of file ids for the pure test data set.

\item {} 
\sphinxstylestrong{dev\_test\_set} \sphinxstyleemphasis{(np.array)} - array(s) with spectrograms of development test data set.

\item {} 
\sphinxstylestrong{fid\_dev\_set} \sphinxstyleemphasis{(list)} - list(s) of file ids for the development test data set.

\end{itemize}


\end{description}\end{quote}

\end{fulllineitems}

\index{run() (generate\_dataset.GenerateDataset method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{generate_dataset:generate_dataset.GenerateDataset.run}}\pysiglinewithargsret{\sphinxbfcode{run}}{\emph{type\_run}}{}
This method extracts the features for the training, validation,
development test, and pure test data sets and saves the resulting tensors
to train and evaluate the convolutional neural network. It uses the 
\sphinxstyleemphasis{multiprocessing} package and the amount of processes created depends on the
quantity of multiprocessing.cpu\_count() function. It has been created for the
normal model (multi\_input = \sphinxstylestrong{False}).
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{type\_run} (\sphinxstyleliteralemphasis{str}) \textendash{} generates ‘train’ or ‘test’ data sets.

\end{description}\end{quote}

\end{fulllineitems}

\index{run\_multi() (generate\_dataset.GenerateDataset method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{generate_dataset:generate_dataset.GenerateDataset.run_multi}}\pysiglinewithargsret{\sphinxbfcode{run\_multi}}{\emph{type\_run}}{}
This method extracts the features for the training, validation,
development test, and pure test data sets and saves the resulting tensors
to train and evaluate the convolutional neural network. It uses the 
\sphinxstyleemphasis{multiprocessing} package and the amount of processes created depends on the
quantity of multiprocessing.cpu\_count() function. It has been created for the
two branch model (multi\_input = \sphinxstylestrong{True}).
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{type\_run} (\sphinxstyleliteralemphasis{str}) \textendash{} generates ‘train’ or ‘test’ data sets.

\end{description}\end{quote}

\end{fulllineitems}

\index{validation\_split() (generate\_dataset.GenerateDataset method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{generate_dataset:generate_dataset.GenerateDataset.validation_split}}\pysiglinewithargsret{\sphinxbfcode{validation\_split}}{\emph{dataset}, \emph{class\_id}, \emph{file\_id}, \emph{num\_classes}, \emph{validation}}{}
This method shuffles the samples randomly and creates a balanced train-validation split depending on 
the total number of examples from each class. This method is only available
for training data sets and can manage to output a single tensor (for the normal model - multi\_input = \sphinxstylestrong{False}) and double tensors
(for the model with two branches - multi\_input = \sphinxstylestrong{True}).
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{dataset} (\sphinxstyleliteralemphasis{list}) \textendash{} list of extracted melspectrograms for the dataset.

\item {} 
\sphinxstyleliteralstrong{class\_id} (\sphinxstyleliteralemphasis{list}) \textendash{} list of labels from the extracted spectrograms.

\item {} 
\sphinxstyleliteralstrong{file\_id} (\sphinxstyleliteralemphasis{list}) \textendash{} file identifier from which the spectrogram was extracted.

\item {} 
\sphinxstyleliteralstrong{num\_classes} (\sphinxstyleliteralemphasis{int}) \textendash{} number of classes in the dataset.

\item {} 
\sphinxstyleliteralstrong{validation} (\sphinxstyleliteralemphasis{float}) \textendash{} train-validation split from 0 to 1.

\end{itemize}

\item[{Returns}] \leavevmode
\begin{itemize}
\item {} 
\sphinxstylestrong{X\_train} \sphinxstyleemphasis{(np.array)} - array(s) with the training data set of spectrograms.

\item {} 
\sphinxstylestrong{y\_train} \sphinxstyleemphasis{(np.array)} - array(s) with training labels of training data set.

\item {} 
\sphinxstylestrong{fid\_train} \sphinxstyleemphasis{(list)} - list(s) of file ids for the training data set.

\item {} 
\sphinxstylestrong{X\_val} \sphinxstyleemphasis{(np.array)} - array(s) with validation data set of spectrograms.

\item {} 
\sphinxstylestrong{Y\_val} \sphinxstyleemphasis{(np.array)} - array(s) with validation labels of validation dataset

\item {} 
\sphinxstylestrong{fid\_val} \sphinxstyleemphasis{(list)} - list(s) of file ids for the validation data set.

\end{itemize}


\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\section{Train the Neural Network}
\label{\detokenize{train::doc}}\label{\detokenize{train:train-the-neural-network}}

\subsection{Usage}
\label{\detokenize{train:usage}}
After creating the tensors that will be used to train and test the neural network, it is
possible to start training the neural network. Different models and optimizers can be selected
by command-line argument parsing. Additionally, the \sphinxstyleemphasis{class\_weight} implementation from \sphinxstyleemphasis{Keras}
can be activated. This was implemented to take into account the uneven distribution of samples
across instrument labels. Given that this did not result in improvements with the IRMAS data
set, the use of this feature is optional in argument parsing. \sphinxstyleemphasis{Since the size of the neural
network and the amount of data, it is strongly recommended to use a GPU to reduce training time.}

Available model names are:
\begin{itemize}
\item {} 
\sphinxstylestrong{model\_baseline} - Baseline model by Han et al. using ReLU activation functions (refer to \sphinxstylestrong{section 4.1}).

\item {} 
\sphinxstylestrong{model\_leaky} - Baseline model by Han et al. using Leaky ReLU activation functions (refer to \sphinxstylestrong{section 4.1}).

\item {} 
\sphinxstylestrong{model\_two\_branch} - Model that allows for two simultaneous inputs and late fusion (refer to \sphinxstylestrong{section 5.6.2}).

\item {} 
\sphinxstylestrong{model\_multi\_res} - Model that allows for three dimensional input (refer to \sphinxstylestrong{section 5.6.3}).

\end{itemize}

Available optimizer names are:
\begin{itemize}
\item {} 
\sphinxstylestrong{adam} - Adam optimizer with parameters beta\_1=0.9, beta\_2=0.999, epsilon=1e-08.

\item {} 
\sphinxstylestrong{sgd} - Stochastic Gradient Descent optimizer with parameters momentum=0.9, decay=learning\_rate/nb\_epochs, Nesterov=True.

\end{itemize}

To train the network, use the following command:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{python3} \PYG{n}{train}\PYG{o}{.}\PYG{n}{py} \PYG{o}{\PYGZhy{}}\PYG{n}{m} \PYG{o}{\PYGZlt{}}\PYG{n}{model\PYGZus{}name}\PYG{o}{\PYGZgt{}} \PYG{o}{\PYGZhy{}}\PYG{n}{o} \PYG{o}{\PYGZlt{}}\PYG{n}{optimizer\PYGZus{}name}\PYG{o}{\PYGZgt{}} \PYG{o}{\PYGZhy{}}\PYG{n}{c} \PYG{o}{\PYGZlt{}}\PYG{n}{y}\PYG{o}{/}\PYG{n}{n}\PYG{o}{\PYGZgt{}}
\end{sphinxVerbatim}

Finally, the resulting files are stored in the MODEL\_PATH defined in \sphinxstyleemphasis{settings.py}:
\begin{itemize}
\item {} 
\sphinxstylestrong{\textless{}model\_name\textgreater{}.\textless{}optimizer\_name\textgreater{}\textless{}iteration\textgreater{}.history.npy} - Training history to evaluate training and validation loss and accuracy.

\item {} 
\sphinxstylestrong{\textless{}model\_name\textgreater{}.\textless{}optimizer\_name\textgreater{}\textless{}iteration\textgreater{}.best.hdf5} - Stored weights by using ModelCheckpoint saving the model with best validation accuracy.

\item {} 
\sphinxstylestrong{\textless{}model\_name\textgreater{}.\textless{}optimizer\_name\textgreater{}\textless{}iteration\textgreater{}.json} - Structure of the neural network.

\end{itemize}


\subsection{Documentation}
\label{\detokenize{train:documentation}}\phantomsection\label{\detokenize{train:module-train}}\index{train (module)}\index{Trainer (class in train)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{train:train.Trainer}}\pysiglinewithargsret{\sphinxbfcode{class }\sphinxcode{train.}\sphinxbfcode{Trainer}}{\emph{model}, \emph{optimizer}, \emph{data\_dist}}{}
Trainer trains a convolutional neural network with a given model. The training data set
must have already been extracted with \sphinxstyleemphasis{generate\_dataset.py}. The balanced train-validation
split is already performed during the data set generation, so the trainer only performs
a randoms shuffling on every epoch. All of the parameters are inherited from \sphinxstyleemphasis{settings.py}.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{model} (\sphinxstyleliteralemphasis{object}) \textendash{} model to be trained with the selected data set.

\item {} 
\sphinxstyleliteralstrong{optimizer} (\sphinxstyleliteralemphasis{str}) \textendash{} optimizer to be used in backpropagation.

\item {} 
\sphinxstyleliteralstrong{data\_dist} (\sphinxstyleliteralemphasis{pandas dataframe}) \textendash{} dataframe with samples distribution in dataset (if class\_weight is implemented).

\end{itemize}

\end{description}\end{quote}
\index{load\_data() (train.Trainer method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{train:train.Trainer.load_data}}\pysiglinewithargsret{\sphinxbfcode{load\_data}}{}{}
This method loads the training and validation tensors (melspectrogram arrays) and 
corresponding labels to memory (for the case of single input). It also implements one hot encoding for the labels
(both training and validation data sets).

\end{fulllineitems}

\index{load\_multi\_data() (train.Trainer method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{train:train.Trainer.load_multi_data}}\pysiglinewithargsret{\sphinxbfcode{load\_multi\_data}}{}{}
This method loads the training and validation tensors (melspectrogram arrays) and 
corresponding labels to memory (for the case of multiple input). It also implements one hot encoding for the labels
(both training and validation data sets).

\end{fulllineitems}

\index{train() (train.Trainer method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{train:train.Trainer.train}}\pysiglinewithargsret{\sphinxbfcode{train}}{\emph{epochs}, \emph{batch}, \emph{iter\_num}}{}
This method trains the corresponding model and implements two callback
functions: Early Stopping (patience = 5 epochs) and Model Checkpointing. The number of epochs and
mini-batch size are inherited from \sphinxstyleemphasis{settings.py}. Additionally, the training history, weights, and 
structure are stored in the MODEL\_PATH.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{epoch} (\sphinxstyleliteralemphasis{int}) \textendash{} maximum number of epochs to train model.

\item {} 
\sphinxstyleliteralstrong{batch} (\sphinxstyleliteralemphasis{int}) \textendash{} mini-batch size.

\item {} 
\sphinxstyleliteralstrong{iter\_num} (\sphinxstyleliteralemphasis{int}) \textendash{} number of training iterations to compensate for random initialization.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\chapter{Evaluation and Results}
\label{\detokenize{index:evaluation-and-results}}

\section{Evaluate the Performance}
\label{\detokenize{evaluate:evaluate-the-performance}}\label{\detokenize{evaluate::doc}}

\subsection{Usage}
\label{\detokenize{evaluate:usage}}
After creating the tensors that will be used to train and test the neural network, it is
possible to start training the neural network. Different models and optimizers can be selected
by command-line argument parsing. Additionally, the \sphinxstylestrong{class\_weight} implementation from \sphinxstyleemphasis{Keras}
can be activated. This was implemented to take into account the uneven distribution of samples
across instrument labels. Given that this did not result in improvements with the IRMAS data
set, the use of this feature is optional in argument parsing.

Available models are: \sphinxstylestrong{model\_baseline}, \sphinxstylestrong{model\_leaky}, \sphinxstylestrong{model\_two\_branch}, and
\sphinxstylestrong{model\_multi\_res}. Available optimizers are: \sphinxstylestrong{adam} and \sphinxstylestrong{sgd}

To train the network, use the following command:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{python3} \PYG{n}{evaluate}\PYG{o}{.}\PYG{n}{py} \PYG{o}{\PYGZhy{}}\PYG{n}{m} \PYG{o}{\PYGZlt{}}\PYG{n}{model\PYGZus{}name}\PYG{o}{\PYGZgt{}} \PYG{o}{\PYGZhy{}}\PYG{n}{o} \PYG{o}{\PYGZlt{}}\PYG{n}{optimizer\PYGZus{}name}\PYG{o}{\PYGZgt{}} \PYG{o}{\PYGZhy{}}\PYG{n}{O} \PYG{o}{\PYGZlt{}}\PYG{l+m+mi}{1}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{o}{/}\PYG{l+m+mi}{3}\PYG{o}{\PYGZgt{}}
\end{sphinxVerbatim}

Depending on the operation mode, follow the instructions to select the split.


\subsection{Operation Mode 1}
\label{\detokenize{evaluate:operation-mode-1}}
This operation mode is used to extract confusion matrices. Since confusion matrices are only
valid for single labeled data, there are two possibilities depending on the used data sets:
\begin{itemize}
\item {} 
\sphinxstylestrong{IRMAS and IRMAS Wind data sets:} since these data sets are multi-labeled on the testing data, a new model is trained by using 50\% of the training data set and the remain for confusion matrix calculation.

\item {} 
\sphinxstylestrong{Monotimbral and Jazz data sets:} since these data sets are single-labeled in both training and testing data sets, confusion matrices can be extracted by using the pure test data set.

\end{itemize}

To plot the confusion matrices:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{cd} \PYG{o}{\PYGZlt{}}\PYG{n}{MODEL\PYGZus{}PATH}\PYG{o}{\PYGZgt{}}\PYG{o}{/}\PYG{o}{\PYGZlt{}}\PYG{n}{model\PYGZus{}name}\PYG{o}{\PYGZgt{}}
\PYG{n}{python3} \PYG{n}{plot\PYGZus{}conf\PYGZus{}mat}\PYG{o}{.}\PYG{n}{py}
\end{sphinxVerbatim}


\subsection{Operation Mode 2}
\label{\detokenize{evaluate:operation-mode-2}}
This operation mode evaluates global and class-wise performance metrics while varying the
identification threshold from 0 to 1. Following the authors, the development test data set
was used to calculate the performance metrics.

To be able to extract the final performance metrics, it is necessary to plot their behaviour
with respect to the identification threshold:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{cd} \PYG{o}{\PYGZlt{}}\PYG{n}{MODEL\PYGZus{}PATH}\PYG{o}{\PYGZgt{}}\PYG{o}{/}\PYG{o}{\PYGZlt{}}\PYG{n}{model\PYGZus{}name}\PYG{o}{\PYGZgt{}}
\PYG{n}{python3} \PYG{n}{plot\PYGZus{}perf\PYGZus{}metrics}\PYG{o}{.}\PYG{n}{py}
\end{sphinxVerbatim}

\sphinxstylestrong{Do not skip this step, before extracting final performance metrics.}


\subsection{Operation Mode 3}
\label{\detokenize{evaluate:operation-mode-3}}
Finally, the final performance metrics are extracted by using the optima identification thresholds
(for both global and class-wise cases) on the pure test data set.


\subsection{Documentation}
\label{\detokenize{evaluate:documentation}}\phantomsection\label{\detokenize{evaluate:module-evaluate}}\index{evaluate (module)}\index{Evaluator (class in evaluate)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{evaluate:evaluate.Evaluator}}\pysiglinewithargsret{\sphinxbfcode{class }\sphinxcode{evaluate.}\sphinxbfcode{Evaluator}}{\emph{model\_str}, \emph{optimizer\_str}, \emph{num\_classes}, \emph{iter\_num}, \emph{op\_mode}}{}
Evaluator uses the testing data set to reproduce results from the 
original neural network design and evaluate the improvements made by 
audio source separation and different proposed experiments. This testing
algorithm allows variable length of audio excerpts.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{model} (\sphinxstyleliteralemphasis{str}) \textendash{} model to be evaluated.

\item {} 
\sphinxstyleliteralstrong{optimizer} (\sphinxstyleliteralemphasis{str}) \textendash{} optimizer that was used in training.

\item {} 
\sphinxstyleliteralstrong{num\_classes} (\sphinxstyleliteralemphasis{int}) \textendash{} number of classes that were trained in the model.

\item {} 
\sphinxstyleliteralstrong{iter\_num} (\sphinxstyleliteralemphasis{int}) \textendash{} number of training iterations to evaluate.

\item {} 
\sphinxstyleliteralstrong{op\_mode} (\sphinxstyleliteralemphasis{int}) \textendash{} operation mode {[}1{]} evaluate confusion matrix, {[}2{]} evaluate classwise and global performance metrics and {[}3{]} final performance metrics using the pure test data set.

\end{itemize}

\end{description}\end{quote}
\index{aggregate\_predictions() (evaluate.Evaluator method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{evaluate:evaluate.Evaluator.aggregate_predictions}}\pysiglinewithargsret{\sphinxbfcode{aggregate\_predictions}}{\emph{strategy}, \emph{full\_predictions}}{}
This method performs different aggregation strategies to obtain a final
prediction for each sample (complete audio excerpt) in the test data set.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{strategy} (\sphinxstyleliteralemphasis{str}) \textendash{} name of the aggregation strategy to evaluate.

\item {} 
\sphinxstyleliteralstrong{full\_predictions} (\sphinxstyleliteralemphasis{list}) \textendash{} complete predictions over all excerpts in test data set.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{best\_classwise\_global\_performance\_metrics() (evaluate.Evaluator method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{evaluate:evaluate.Evaluator.best_classwise_global_performance_metrics}}\pysiglinewithargsret{\sphinxbfcode{best\_classwise\_global\_performance\_metrics}}{\emph{strategy}, \emph{iteration}}{}
This method calculates the global performance metrics of the system, 
using a novel approach: using the optima class-wise thresholds in
in the pure test data set. It calculates precision, recall and f-score for micro and macro averaging.
It also evaluates different performance metrics using a variable threshold 
for each instrument separately.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{strategy} (\sphinxstyleliteralemphasis{str}) \textendash{} name of the aggregation strategy to store.

\item {} 
\sphinxstyleliteralstrong{iteration} (\sphinxstyleliteralemphasis{int}) \textendash{} id of the iteration being evaluated.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{classwise\_performance\_metrics() (evaluate.Evaluator method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{evaluate:evaluate.Evaluator.classwise_performance_metrics}}\pysiglinewithargsret{\sphinxbfcode{classwise\_performance\_metrics}}{\emph{strategy}, \emph{iteration}}{}
This method calculates the classwise performance metrics of the system. It
calculates precision, recall and f-score for micro and macro averaging.
It also evaluates different performance metrics using a variable threshold 
for each instrument separately.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{strategy} (\sphinxstyleliteralemphasis{str}) \textendash{} name of the aggregation strategy to store.

\item {} 
\sphinxstyleliteralstrong{iteration} (\sphinxstyleliteralemphasis{int}) \textendash{} id of the iteration being evaluated.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{evaluate() (evaluate.Evaluator method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{evaluate:evaluate.Evaluator.evaluate}}\pysiglinewithargsret{\sphinxbfcode{evaluate}}{\emph{predictions}, \emph{model\_str}, \emph{optimizer\_str}, \emph{iteration}}{}
This method evaluates the corresponding trained model i with different
aggregation strategies for each sample in the dataset. It saves a dictionary
with the corresponding performance metrics depending on the operation mode
of the evaluator and the incoming predicted values.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{predictions} (\sphinxstyleliteralemphasis{list}) \textendash{} calculated predictions for model i and selected operation mode.

\item {} 
\sphinxstyleliteralstrong{model\_str} (\sphinxstyleliteralemphasis{str}) \textendash{} name of the model to load.

\item {} 
\sphinxstyleliteralstrong{optimizer\_str} (\sphinxstyleliteralemphasis{str}) \textendash{} name of the optimizer used in training.

\item {} 
\sphinxstyleliteralstrong{iteration} (\sphinxstyleliteralemphasis{int}) \textendash{} id of the iteration being evaluated.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{global\_performance\_metrics() (evaluate.Evaluator method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{evaluate:evaluate.Evaluator.global_performance_metrics}}\pysiglinewithargsret{\sphinxbfcode{global\_performance\_metrics}}{\emph{strategy}, \emph{iteration}}{}
This method calculates the global performance metrics of the system. It
calculates precision, recall and f-score for micro and macro averaging.
It also evaluates different performance metrics using a variable global threshold 
for all instruments.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{strategy} (\sphinxstyleliteralemphasis{str}) \textendash{} name of the aggregation strategy to store.

\item {} 
\sphinxstyleliteralstrong{iteration} (\sphinxstyleliteralemphasis{int}) \textendash{} id of the iteration being evaluated.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{load\_model() (evaluate.Evaluator method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{evaluate:evaluate.Evaluator.load_model}}\pysiglinewithargsret{\sphinxbfcode{load\_model}}{\emph{model\_str}, \emph{optimizer\_str}}{}
This method loads the model and the weights obtained in training.
Each model is represented by the activation function used and the optimizer 
used in backpropagation. It also calculates the predictions for every
melspectrogram in the testing data sets (validation, development, and pure test data sets).
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{model\_str} (\sphinxstyleliteralemphasis{str}) \textendash{} name of the model to load.

\item {} 
\sphinxstyleliteralstrong{optimizer\_str} (\sphinxstyleliteralemphasis{str}) \textendash{} name of the optimizer used in training.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{load\_multi\_test\_data() (evaluate.Evaluator method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{evaluate:evaluate.Evaluator.load_multi_test_data}}\pysiglinewithargsret{\sphinxbfcode{load\_multi\_test\_data}}{}{}
This method loads the multiple-input development test data (melspectrogram arrays) and 
corresponding file ids to memory. It also implements one hot encoding 
and multilabel binarizer for the labels in the test data set. The loaded data set
depends on the operation mode selected to perform the evaluation: {[}1{]} loads 50-50
train-validation split, {[}2{]} loads variable train-validation split, {[}3{]} loads pure 
testing data set.

\end{fulllineitems}

\index{load\_test\_data() (evaluate.Evaluator method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{evaluate:evaluate.Evaluator.load_test_data}}\pysiglinewithargsret{\sphinxbfcode{load\_test\_data}}{}{}
This method loads the single-input development test data (melspectrogram arrays) and 
corresponding file ids to memory. It also implements one hot encoding 
and multilabel binarizer for the labels in the test data set. The loaded data set
depends on the operation mode selected to perform the evaluation: {[}1{]} loads 50-50
train-validation split, {[}2{]} loads variable train-validation split, {[}3{]} loads pure 
testing data set.

\end{fulllineitems}

\index{save\_results() (evaluate.Evaluator method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{evaluate:evaluate.Evaluator.save_results}}\pysiglinewithargsret{\sphinxbfcode{save\_results}}{\emph{model\_str}, \emph{optimizer\_str}}{}
This method saves the results of the evaluation depending on the operation mode
of the evaluator.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{model\_str} (\sphinxstyleliteralemphasis{str}) \textendash{} name of the model

\item {} 
\sphinxstyleliteralstrong{optimizer\_str} (\sphinxstyleliteralemphasis{str}) \textendash{} name of the optimizer

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\section{Comparing Results}
\label{\detokenize{results:comparing-results}}\label{\detokenize{results::doc}}

\subsection{Usage}
\label{\detokenize{results:usage}}
The final results obtained by the evaluation script can now be compared among all
trained models. To do this interactively, use the following commands:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{cd} \PYG{n}{models}
\PYG{n}{ipython3} \PYG{n}{compare\PYGZus{}models}\PYG{o}{.}\PYG{n}{py}
\end{sphinxVerbatim}


\subsection{Review Results}
\label{\detokenize{results:review-results}}
The results of performance metrics are stored to Pandas data frames. Results are stored
in the following \sphinxstyleemphasis{Pandas} dataframes:
\begin{itemize}
\item {} 
\sphinxstylestrong{global\_res:} Global performance metrics.

\item {} 
\sphinxstylestrong{class\_res:} Class-wise performance metrics.

\item {} 
\sphinxstylestrong{global\_class\_res:} Global performance metrics by using optima class-wise thresholds.

\end{itemize}

By accessing it interactively, it is possible to find the global performance
metrics with respect to the data set, model, aggregation strategy and averaging
method used.

For example:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{global\PYGZus{}res}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{irmas\PYGZus{}1024}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{model\PYGZus{}baseline}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{s2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{micro}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\PYG{n}{class\PYGZus{}res}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{jazz\PYGZus{}db\PYGZus{}1024}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{model\PYGZus{}baseline}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{s1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{voi}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\PYG{n}{global\PYGZus{}class\PYGZus{}res}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{youtube\PYGZus{}1024}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{model\PYGZus{}leaky}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{s1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{macro}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\end{sphinxVerbatim}

will print the precision, recall, and f-scores of these cases.

Finally, you can uncomment sections of the script to plot the improvements by comparing
the models in between them.


\chapter{References}
\label{\detokenize{index:references}}

\chapter{Indices and tables}
\label{\detokenize{index:indices-and-tables}}\begin{itemize}
\item {} 
\DUrole{xref,std,std-ref}{genindex}

\item {} 
\DUrole{xref,std,std-ref}{modindex}

\item {} 
\DUrole{xref,std,std-ref}{search}

\end{itemize}


\renewcommand{\indexname}{Python Module Index}
\begin{sphinxtheindex}
\def\bigletter#1{{\Large\sffamily#1}\nopagebreak\vspace{1mm}}
\bigletter{e}
\item {\sphinxstyleindexentry{evaluate}}\sphinxstyleindexpageref{evaluate:\detokenize{module-evaluate}}
\indexspace
\bigletter{g}
\item {\sphinxstyleindexentry{generate\_dataset}}\sphinxstyleindexpageref{generate_dataset:\detokenize{module-generate_dataset}}
\indexspace
\bigletter{t}
\item {\sphinxstyleindexentry{train}}\sphinxstyleindexpageref{train:\detokenize{module-train}}
\end{sphinxtheindex}

\renewcommand{\indexname}{Index}
\printindex
\end{document}